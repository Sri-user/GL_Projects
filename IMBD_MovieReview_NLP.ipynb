{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xT7MKZuMRaCg"
   },
   "source": [
    "# Sentiment Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wq4RCyyPSYRp"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras import backend as K\n",
    "\n",
    "seed=999\n",
    "vocab_size = 10000\n",
    "maxlen = 500  #number of word to be used from each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NGCtiXUhSWss"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size,seed=seed) # vocab_size is no.of words to consider from the dataset, ordering based on frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000,)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "randomNumber = np.random.randint(0,len(X)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2494\n"
     ]
    }
   ],
   "source": [
    "max_size=0\n",
    "for i in range(0,len(X)-1):\n",
    "    size= len(X[i])\n",
    "    if(size>=max_size):\n",
    "        max_size=size\n",
    "print(max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4053\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in range(0,len(X)-1):\n",
    "    size= len(X[i])\n",
    "    if(size>=500):\n",
    "        count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The highest count of words in a review is 2494. Truncating such a large comment is a loss of information.\n",
    "\n",
    "# However reviews with words greater than 500 is less than 10 percent of the total reviews. So we can set this number to truncate the words in the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y)) #Binary classification for bad and good comments respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9998\n"
     ]
    }
   ],
   "source": [
    "print(len(np.unique(np.hstack(X)))) #After limitting words with vocabulary size, number of words are just close to 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the and gas to about whole he romantic true cast movie about and origins help spots be couple drawing brilliance actors work would it this about commits who's example length they about help but were dressing and about performed it and no about killer it escape unhinged in is and but and about pulp choreography charge and in and some they about help to sun glasses this described of worse couple complex recreate movie much movie of we're br of performances captures like short in person this about personality everyone i i of their it 1994 in is anne if horrific cops to which special make fi something more interesting think uses of before embarrassing of indeed he sound and adaptations it monster surrounding movie of somewhere killer to protect it happened depth movie of terrifying chapter killer br of performances captures not faye and movie and it movies walter in tears of on of period it time honestly to and big world man's that better would there about understand to no would monsters any would well at very not more than movies plot gets no would moments better reaction by well can about girls as it when only top it to as it of school watching absolutely in girl been look and in of because before evil his of see by field and movie recreate to and hospital movie be editing mechanical other achieve or boom reveals not they there's enjoyed this shrill something i i what bit times of dr after one way about so bonanza on to that there that understand not get br that it and things film so walter michael probably to sadly development\n"
     ]
    }
   ],
   "source": [
    "# Get the actual review\n",
    "index = imdb.get_word_index()\n",
    "reverse_index = dict([(value, key) for (key, value) in index.items()]) \n",
    "decoded = \" \".join( [reverse_index.get(i, \"#\") for i in X[randomNumber]] )\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fCPC_WN-eCyw"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qMEsHYrWxdtk"
   },
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h0g381XzeCyz"
   },
   "outputs": [],
   "source": [
    "#make all sequences of the same length\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen,padding='post')\n",
    "X_test =  pad_sequences(X_test, maxlen=maxlen,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jy6n-uM2eCy2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,    4,    2,    7,    2, 5834,   11,    4, 8064,   70,    2,\n",
       "          6,    2,   11,    4, 6024, 1793,    8,   14, 2604,  820, 2472,\n",
       "          8,    4, 4117, 2604,  146,   24,  252,  618,   89,  175,  206,\n",
       "         57,  551,   89,  392,   42, 8966,   80,  380,    6, 3629, 2097,\n",
       "         15,   70,  485,    8,  194,  687,   14,  392, 6823,    7,    6,\n",
       "         22,  287,  178,    6,  201,    7, 1573, 5684,  105,   91,    7,\n",
       "         98,   11, 1450,  625, 1729,   80, 4525,  257,    2,  456,    4,\n",
       "       1446, 7079,    7,    2, 2604,   70,   30, 7452,   17,   73,  257,\n",
       "         65,    9,    6,    2,    7,    4, 6823,   15,    9,   14,   22,\n",
       "          4,  116,    9, 8625, 6858,    5, 7468, 5860,    2,    9,    2,\n",
       "          4,  537,    2,    2,    4,  486,    9,   43,  208,    5,    4,\n",
       "        529,  889,    4, 1716,    2,   11,    2,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dybtUgUReCy8"
   },
   "source": [
    "## Build Keras Embedding Layer Model\n",
    "We can think of the Embedding layer as a dicionary that maps a index assigned to a word to a word vector. This layer is very flexible and can be used in a few ways:\n",
    "\n",
    "* The embedding layer can be used at the start of a larger deep learning model. \n",
    "* Also we could load pre-train word embeddings into the embedding layer when we create our model.\n",
    "* Use the embedding layer to train our own word2vec models.\n",
    "\n",
    "The keras embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unqiue intger number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A5OLM4eBeCy9"
   },
   "outputs": [],
   "source": [
    "# Only the top 10000 words are loaded, with vocab_size=10000\n",
    "# Also truncating words in each review to 300, truncating longer reviews and zero padding shorter reviews with pad_sequences\n",
    "# And we are creating a dimensions of 32, 50 or 100 for each word, and compare the performance of the same\n",
    "\n",
    "#n_dim=[32,50,100] - A 50 dimension embedding provides better result in our model, as we looped through these and compared the accuracy\n",
    "n_dim=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TxNDNhrseCzA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 500, 50)           500000    \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 25000)             0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 200)               5000200   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 5,520,401\n",
      "Trainable params: 5,520,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#for dim in n_dim:   \n",
    "\n",
    "# create the model\n",
    "\n",
    "model = Sequential()\n",
    "    #model.add(Embedding(vocab_size, dim, input_length=maxlen))\n",
    "model.add(Embedding(vocab_size, n_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srith\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      " - 13s - loss: 0.4963 - accuracy: 0.7261 - val_loss: 0.3276 - val_accuracy: 0.8582\n",
      "Epoch 2/2\n",
      " - 12s - loss: 0.1479 - accuracy: 0.9454 - val_loss: 0.3557 - val_accuracy: 0.8610\n",
      "Accuracy: 86.10%\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "    # Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L3CSVVPPeCzD"
   },
   "outputs": [],
   "source": [
    "# A 50 dimensional vector size for each word is providing a better accuracy than a 50D or 300D model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even training in two epochs, there is a overfitting issue.Training with 5 epochs make the training accuracy close to 100 percent\n",
    "# Hence,We will limit the epochs to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viewing the actual and predicted output for the review ID: 2017\n",
      "Actual Output: 0\n",
      "Predicted Output: [0.00551283]\n",
      "Review has been predicted as expected\n",
      "\n",
      "\n",
      "Viewing the actual and predicted output for the review ID: 3142\n",
      "Actual Output: 1\n",
      "Predicted Output: [0.8275218]\n",
      "Review has been predicted as expected\n",
      "\n",
      "\n",
      "Viewing the actual and predicted output for the review ID: 3031\n",
      "Actual Output: 1\n",
      "Predicted Output: [0.9973098]\n",
      "Review has been predicted as expected\n",
      "\n",
      "\n",
      "Viewing the actual and predicted output for the review ID: 1178\n",
      "Actual Output: 0\n",
      "Predicted Output: [0.7256348]\n",
      "Wrong prediction of review\n",
      "\n",
      "\n",
      "Viewing the actual and predicted output for the review ID: 13060\n",
      "Actual Output: 0\n",
      "Predicted Output: [0.00030848]\n",
      "Review has been predicted as expected\n",
      "\n",
      "\n",
      "Viewing the actual and predicted output for the review ID: 19459\n",
      "Actual Output: 1\n",
      "Predicted Output: [0.9786644]\n",
      "Review has been predicted as expected\n",
      "\n",
      "\n",
      "Viewing the actual and predicted output for the review ID: 8139\n",
      "Actual Output: 1\n",
      "Predicted Output: [0.9982181]\n",
      "Review has been predicted as expected\n",
      "\n",
      "\n",
      "Viewing the actual and predicted output for the review ID: 19586\n",
      "Actual Output: 1\n",
      "Predicted Output: [0.98971105]\n",
      "Review has been predicted as expected\n",
      "\n",
      "\n",
      "Viewing the actual and predicted output for the review ID: 2651\n",
      "Actual Output: 0\n",
      "Predicted Output: [0.04531017]\n",
      "Review has been predicted as expected\n",
      "\n",
      "\n",
      "Viewing the actual and predicted output for the review ID: 22481\n",
      "Actual Output: 0\n",
      "Predicted Output: [0.02262691]\n",
      "Review has been predicted as expected\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "predicted_labels=model.predict(X_test)\n",
    "while(i<10):\n",
    "    test_randomNumber = np.random.randint(0,y_test.shape[0]-1)\n",
    "    print(\"Viewing the actual and predicted output for the review ID: {}\".format(test_randomNumber))\n",
    "    print(\"Actual Output: {}\".format(y_test[test_randomNumber]))\n",
    "    print(\"Predicted Output: {}\".format(predicted_labels[test_randomNumber]))\n",
    "    \n",
    "    if(y_test[test_randomNumber]==0) and (predicted_labels[test_randomNumber]<0.5):\n",
    "        print(\"Review has been predicted as expected\")\n",
    "    elif(y_test[test_randomNumber]==0) and (predicted_labels[test_randomNumber]>=0.5):\n",
    "        print(\"Wrong prediction of review\")\n",
    "    elif(y_test[test_randomNumber]==1) and (predicted_labels[test_randomNumber]<0.5):\n",
    "        print(\"Wrong prediction of review\")\n",
    "    elif(y_test[test_randomNumber]==1) and (predicted_labels[test_randomNumber]>=0.5):\n",
    "        print(\"Review has been predicted as expected\")\n",
    "    else:\n",
    "        print(\"Check the Predicted and actual value\")\n",
    "    print(\"\\n\")\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see, the predicted outputs are close to 1, and viceversa for the actual outputs to be 1 and 0 respectively\n",
    "# However some results vary, as we yet have only 86 percent accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[ 4.2039114e-03  8.1935379e-04 -6.6072680e-05 ...  5.8760040e-04\n",
      " -1.1084182e-03  4.0380177e-03]\n",
      "\n",
      "\n",
      "[ 2.73373991e-01 -0.00000000e+00  4.32829946e-01 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  4.03797597e-01 -0.00000000e+00  1.18092686e-01 -0.00000000e+00\n",
      "  3.23214568e-02 -0.00000000e+00  6.74725100e-02 -0.00000000e+00\n",
      "  1.29054219e-01  2.94829458e-01  3.74431312e-01  2.10190639e-01\n",
      " -0.00000000e+00  2.22499892e-02  3.00118178e-02 -0.00000000e+00\n",
      "  1.28013834e-01  2.02649802e-01  5.88609397e-01  8.00798386e-02\n",
      " -0.00000000e+00  5.12947440e-01 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00  1.64791584e-01\n",
      " -0.00000000e+00  2.64106303e-01  2.74552733e-01  2.55601287e-01\n",
      "  2.70067722e-01 -0.00000000e+00  3.97487521e-01 -0.00000000e+00\n",
      "  3.12402636e-01 -0.00000000e+00 -0.00000000e+00  2.01763660e-01\n",
      "  1.28781959e-01  1.66037038e-01  1.39781684e-01 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  1.00367308e-01 -0.00000000e+00\n",
      "  4.93245870e-01  1.32935166e-01  1.00726765e-02  5.58488294e-02\n",
      "  3.27004403e-01 -0.00000000e+00  3.94637734e-01  4.16780040e-02\n",
      " -0.00000000e+00  1.91925451e-01 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00  1.29618585e-01\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  2.43461989e-02 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  3.70687455e-01 -0.00000000e+00 -0.00000000e+00\n",
      "  6.47464514e-01 -0.00000000e+00  8.71432424e-02 -0.00000000e+00\n",
      " -0.00000000e+00  2.66046962e-03  1.05378293e-01 -0.00000000e+00\n",
      "  1.81889564e-01 -0.00000000e+00 -0.00000000e+00  9.12936404e-02\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  8.64824206e-02 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  1.73350871e-01  2.02749431e-01 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  3.30929160e-02 -0.00000000e+00  4.04798329e-01\n",
      "  9.49056968e-02 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  1.22609511e-01 -0.00000000e+00 -0.00000000e+00\n",
      "  7.04396516e-03 -0.00000000e+00  3.68076384e-01  3.09608996e-01\n",
      " -0.00000000e+00  1.51780739e-01 -0.00000000e+00  3.19503456e-01\n",
      "  3.54541749e-01 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  6.40726313e-02  2.98304781e-02\n",
      "  3.47489476e-01 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  4.52241182e-01 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  2.70676821e-01 -0.00000000e+00  4.37849969e-01\n",
      "  1.29596174e-01 -0.00000000e+00  6.87719733e-02  1.50946692e-01\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  3.34255993e-01 -0.00000000e+00\n",
      "  2.74537355e-01 -0.00000000e+00  4.08729352e-03  9.02731437e-03\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00  5.15194163e-02\n",
      " -0.00000000e+00  2.41378322e-04 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  8.11639726e-02  1.74340829e-02 -0.00000000e+00 -0.00000000e+00\n",
      "  3.06785464e-01  3.16638380e-01  4.01945114e-01  2.93568224e-02]\n",
      "\n",
      "\n",
      "[ 2.73373991e-01 -0.00000000e+00  4.32829946e-01 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  4.03797597e-01 -0.00000000e+00  1.18092686e-01 -0.00000000e+00\n",
      "  3.23214568e-02 -0.00000000e+00  6.74725100e-02 -0.00000000e+00\n",
      "  1.29054219e-01  2.94829458e-01  3.74431312e-01  2.10190639e-01\n",
      " -0.00000000e+00  2.22499892e-02  3.00118178e-02 -0.00000000e+00\n",
      "  1.28013834e-01  2.02649802e-01  5.88609397e-01  8.00798386e-02\n",
      " -0.00000000e+00  5.12947440e-01 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00  1.64791584e-01\n",
      " -0.00000000e+00  2.64106303e-01  2.74552733e-01  2.55601287e-01\n",
      "  2.70067722e-01 -0.00000000e+00  3.97487521e-01 -0.00000000e+00\n",
      "  3.12402636e-01 -0.00000000e+00 -0.00000000e+00  2.01763660e-01\n",
      "  1.28781959e-01  1.66037038e-01  1.39781684e-01 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  1.00367308e-01 -0.00000000e+00\n",
      "  4.93245870e-01  1.32935166e-01  1.00726765e-02  5.58488294e-02\n",
      "  3.27004403e-01 -0.00000000e+00  3.94637734e-01  4.16780040e-02\n",
      " -0.00000000e+00  1.91925451e-01 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00  1.29618585e-01\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  2.43461989e-02 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  3.70687455e-01 -0.00000000e+00 -0.00000000e+00\n",
      "  6.47464514e-01 -0.00000000e+00  8.71432424e-02 -0.00000000e+00\n",
      " -0.00000000e+00  2.66046962e-03  1.05378293e-01 -0.00000000e+00\n",
      "  1.81889564e-01 -0.00000000e+00 -0.00000000e+00  9.12936404e-02\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  8.64824206e-02 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  1.73350871e-01  2.02749431e-01 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  3.30929160e-02 -0.00000000e+00  4.04798329e-01\n",
      "  9.49056968e-02 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  1.22609511e-01 -0.00000000e+00 -0.00000000e+00\n",
      "  7.04396516e-03 -0.00000000e+00  3.68076384e-01  3.09608996e-01\n",
      " -0.00000000e+00  1.51780739e-01 -0.00000000e+00  3.19503456e-01\n",
      "  3.54541749e-01 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  6.40726313e-02  2.98304781e-02\n",
      "  3.47489476e-01 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  4.52241182e-01 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  2.70676821e-01 -0.00000000e+00  4.37849969e-01\n",
      "  1.29596174e-01 -0.00000000e+00  6.87719733e-02  1.50946692e-01\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  3.34255993e-01 -0.00000000e+00\n",
      "  2.74537355e-01 -0.00000000e+00  4.08729352e-03  9.02731437e-03\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00  5.15194163e-02\n",
      " -0.00000000e+00  2.41378322e-04 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  8.11639726e-02  1.74340829e-02 -0.00000000e+00 -0.00000000e+00\n",
      "  3.06785464e-01  3.16638380e-01  4.01945114e-01  2.93568224e-02]\n",
      "\n",
      "\n",
      "[ 0.9352103  -0.         -0.         -0.         -0.          0.5446408\n",
      "  0.30538118 -0.         -0.         -0.          0.9002218  -0.\n",
      "  0.45874938  0.08034912 -0.         -0.         -0.         -0.\n",
      "  0.6668451  -0.         -0.         -0.          0.37496275  0.61055183\n",
      " -0.          0.8002776  -0.         -0.          0.3030225   0.07344413\n",
      " -0.         -0.         -0.         -0.         -0.          0.2579016\n",
      " -0.         -0.          0.45455837 -0.         -0.         -0.\n",
      "  0.6393002   0.41572362  0.79029244 -0.          0.618158   -0.\n",
      "  0.48139346 -0.         -0.          0.86103964 -0.          0.85282356\n",
      "  0.3670706   0.9003925  -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.          0.32488367 -0.\n",
      " -0.         -0.          0.65244436 -0.         -0.         -0.\n",
      " -0.          0.8443083  -0.          1.0801547  -0.          0.70493454\n",
      " -0.         -0.         -0.         -0.          0.94366777 -0.\n",
      " -0.         -0.          1.0845368   0.89202464 -0.         -0.\n",
      " -0.         -0.         -0.          0.46256465  1.1245599   0.8219295\n",
      " -0.         -0.         -0.         -0.        ]\n",
      "\n",
      "\n",
      "[0.02262691]\n"
     ]
    }
   ],
   "source": [
    "for layer in range(1,len(model.layers)):\n",
    "    get_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[layer].output])\n",
    "    layer_output = get_layer_output([X_test])[0]\n",
    "    #print(\"\\nOutput of layer - {} is {}\".format(layer,layer_output))\n",
    "    print(\"\\n\")\n",
    "    print(layer_output[test_randomNumber])\n",
    "    #print(layer_output.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02262691], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_output[test_randomNumber]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02262694]], dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([[X_test[test_randomNumber]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[test_randomNumber]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Thus we have built a NLP binary classification based on the reviews present for the IMDB movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0AqOnLa2eCzH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-dUDSg7VeCzM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tskt_1npeCzP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SeqNLP_Project1_Questions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
